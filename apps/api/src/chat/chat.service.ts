import { Injectable, Logger } from '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import { GoogleGenerativeAI } from '@google/generative-ai';
import { ProductsService } from '../products/products.service';

@Injectable()
export class ChatService {
    private readonly logger = new Logger(ChatService.name);
    private genAI: GoogleGenerativeAI;
    private model: any;

    // Simple in-memory cache for product context
    private productContextCache: { data: string; timestamp: number } | null = null;
    private readonly CACHE_TTL = 60 * 1000; // 60 seconds
    private readonly MAX_PRODUCTS_IN_CONTEXT = 20; // Limit products to save tokens

    constructor(
        private configService: ConfigService,
        private productsService: ProductsService,
    ) {
        const apiKey = this.configService.get<string>('GEMINI_API_KEY');
        if (apiKey) {
            this.genAI = new GoogleGenerativeAI(apiKey);
            this.model = this.genAI.getGenerativeModel({ model: 'gemini-2.5-flash-lite' });
        } else {
            this.logger.warn('GEMINI_API_KEY not found');
        }
    }

    async getProductContext(): Promise<string> {
        const now = Date.now();
        if (this.productContextCache && (now - this.productContextCache.timestamp < this.CACHE_TTL)) {
            return this.productContextCache.data;
        }

        const products = await this.productsService.getProductsForAI();

        // Limit to MAX_PRODUCTS_IN_CONTEXT to save token budget for output
        const limitedProducts = products.slice(0, this.MAX_PRODUCTS_IN_CONTEXT);

        // Format as simplified text
        const textContext = limitedProducts.map(p => {
            return `ID: ${p.id} | ${p.title} | ${p.price} VND | /${p.slug}`;
        }).join('\n');

        this.productContextCache = { data: textContext, timestamp: now };
        return textContext;
    }

    async processMessage(userMessage: string, history: any[]) {
        if (!this.model) {
            return { text: "Xin l·ªói, h·ªá th·ªëng t∆∞ v·∫•n ƒëang b·∫£o tr√¨. Vui l√≤ng th·ª≠ l·∫°i sau.", recommendations: [] };
        }

        try {
            const context = await this.getProductContext();

            const systemPrompt = `B·∫°n l√† chuy√™n gia t∆∞ v·∫•n Tr·∫ßm H∆∞∆°ng Thi√™n Ph√∫c. Phong c√°ch: l·ªãch s·ª±, hi·ªÉu bi·∫øt, Zen. Ng√¥n ng·ªØ: Ti·∫øng Vi·ªát.

S·∫¢N PH·∫®M HI·ªÜN C√ì:
${context}

QUY T·∫ÆC:
1. CH·ªà g·ª£i √Ω s·∫£n ph·∫©m trong danh s√°ch tr√™n.
2. N·∫øu kh√°ch h·ªèi chung chung, h·ªèi l·∫°i M·ª§C ƒê√çCH (x√¥ng nh√†/qu√† t·∫∑ng) ho·∫∑c NG√ÇN S√ÅCH.
3. Khi g·ª£i √Ω c·ª• th·ªÉ, k√®m JSON block:
\`\`\`json
{"recommendations":[{"id":"...","slug":"...","title":"...","price":0,"reason":"l√Ω do ng·∫Øn"}]}
\`\`\`
4. T·ª´ ch·ªëi c√¢u h·ªèi ngo√†i ch·ªß ƒë·ªÅ l·ªãch s·ª±.
`;

            const chat = this.model.startChat({
                history: history.map(h => ({
                    role: h.role === 'user' ? 'user' : 'model',
                    parts: [{ text: h.content }]
                })),
                generationConfig: {
                    temperature: 0.7,
                    maxOutputTokens: 4096, // Increased to 4096 to prevent truncation
                },
                systemInstruction: { role: 'system', parts: [{ text: systemPrompt }] }
            });

            const result = await chat.sendMessage(userMessage);
            const response = await result.response;
            const text = response.text();

            // Log finish reason for monitoring truncation
            const finishReason = response.candidates?.[0]?.finishReason;
            const promptTokens = response.usageMetadata?.promptTokenCount || 0;
            const outputTokens = response.usageMetadata?.candidatesTokenCount || 0;

            if (finishReason === 'MAX_TOKENS') {
                this.logger.warn(`‚ö†Ô∏è Response truncated! Finish: ${finishReason}, Prompt: ${promptTokens}, Output: ${outputTokens}`);
            } else {
                this.logger.log(`‚úÖ Gemini response. Finish: ${finishReason}, Prompt: ${promptTokens}, Output: ${outputTokens}`);
            }

            return {
                raw_response: text,
            };

        } catch (e) {
            this.logger.error('Gemini API Error', e);
            return { text: "Xin l·ªói, t√¥i ƒëang g·∫∑p ch√∫t kh√≥ khƒÉn khi k·∫øt n·ªëi. B·∫°n ch·ªù m·ªôt l√°t nh√©." };
        }
    }

    /**
     * Streaming version of processMessage using sendMessageStream()
     * Yields SSE-formatted events for each chunk
     */
    async *processMessageStream(userMessage: string, history: any[]): AsyncGenerator<string> {
        if (!this.model) {
            yield `data: ${JSON.stringify({ type: 'error', content: 'H·ªá th·ªëng t∆∞ v·∫•n ƒëang b·∫£o tr√¨.' })}\n\n`;
            return;
        }

        try {
            const context = await this.getProductContext();

            const systemPrompt = `B·∫°n l√† chuy√™n gia t∆∞ v·∫•n Tr·∫ßm H∆∞∆°ng Thi√™n Ph√∫c. Phong c√°ch: l·ªãch s·ª±, hi·ªÉu bi·∫øt, Zen. Ng√¥n ng·ªØ: Ti·∫øng Vi·ªát.

S·∫¢N PH·∫®M HI·ªÜN C√ì:
${context}

QUY T·∫ÆC:
1. CH·ªà g·ª£i √Ω s·∫£n ph·∫©m trong danh s√°ch tr√™n.
2. N·∫øu kh√°ch h·ªèi chung chung, h·ªèi l·∫°i M·ª§C ƒê√çCH (x√¥ng nh√†/qu√† t·∫∑ng) ho·∫∑c NG√ÇN S√ÅCH.
3. Khi g·ª£i √Ω c·ª• th·ªÉ, k√®m JSON block:
\`\`\`json
{"recommendations":[{"id":"...","slug":"...","title":"...","price":0,"reason":"l√Ω do ng·∫Øn"}]}
\`\`\`
4. T·ª´ ch·ªëi c√¢u h·ªèi ngo√†i ch·ªß ƒë·ªÅ l·ªãch s·ª±.
`;

            const chat = this.model.startChat({
                history: history.map(h => ({
                    role: h.role === 'user' ? 'user' : 'model',
                    parts: [{ text: h.content }]
                })),
                generationConfig: {
                    temperature: 0.7,
                    maxOutputTokens: 4096,
                },
                systemInstruction: { role: 'system', parts: [{ text: systemPrompt }] }
            });

            this.logger.log('üöÄ Starting stream...');
            const result = await chat.sendMessageStream(userMessage);

            let totalChunks = 0;
            for await (const chunk of result.stream) {
                const chunkText = chunk.text();
                if (chunkText) {
                    totalChunks++;
                    yield `data: ${JSON.stringify({ type: 'chunk', content: chunkText })}\n\n`;
                }
            }

            // Get final response for metadata
            const response = await result.response;
            const finishReason = response.candidates?.[0]?.finishReason;
            const promptTokens = response.usageMetadata?.promptTokenCount || 0;
            const outputTokens = response.usageMetadata?.candidatesTokenCount || 0;

            this.logger.log(`‚úÖ Stream complete. Chunks: ${totalChunks}, Finish: ${finishReason}, Prompt: ${promptTokens}, Output: ${outputTokens}`);

            // Send warning if truncated
            if (finishReason === 'MAX_TOKENS') {
                this.logger.warn(`‚ö†Ô∏è Stream truncated!`);
                yield `data: ${JSON.stringify({ type: 'warning', content: 'C√¢u tr·∫£ l·ªùi c√≥ th·ªÉ b·ªã r√∫t g·ªçn do gi·ªõi h·∫°n ƒë·ªô d√†i.' })}\n\n`;
            }

            // Send done event
            yield `data: ${JSON.stringify({ type: 'done', finishReason })}\n\n`;

        } catch (e) {
            this.logger.error('Gemini Stream Error', e);
            yield `data: ${JSON.stringify({ type: 'error', content: 'Xin l·ªói, c√≥ l·ªói x·∫£y ra khi k·∫øt n·ªëi.' })}\n\n`;
        }
    }
}
